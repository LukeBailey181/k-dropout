{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from plotting_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 97,\n",
       " 'm': 1,\n",
       " 'p': 0.5,\n",
       " 'lr': 0.0005,\n",
       " 'seed': 229,\n",
       " 'device': 'cuda',\n",
       " 'epochs': 3,\n",
       " 'input_p': 0,\n",
       " 'n_hidden': 2,\n",
       " 'run_name': None,\n",
       " 'batch_size': 512,\n",
       " 'input_size': 3072,\n",
       " 'hidden_size': 2000,\n",
       " 'num_workers': 4,\n",
       " 'output_size': 10,\n",
       " 'dataset_name': 'cifar10',\n",
       " 'git_snapshot': 'git_snapshot_f10142_712bf9',\n",
       " 'store_weights': True,\n",
       " 'test_batch_size': 512,\n",
       " 'n_random_subnets': 3,\n",
       " 'skip_mask_performance': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rid = 'wttozyi2'\n",
    "\n",
    "config = get_run_config(rid)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dropout_mask_2.pt',\n",
       " 'weights_epoch_3.pt',\n",
       " 'random_mask_1.pt',\n",
       " 'weights_epoch_2.pt',\n",
       " 'random_mask_0.pt',\n",
       " 'weights_epoch_1.pt',\n",
       " 'weights_epoch_0.pt',\n",
       " 'random_mask_2.pt',\n",
       " 'dropout_mask_0.pt',\n",
       " 'dropout_mask_1.pt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_path = f'../models/{rid}'\n",
    "run_files = os.listdir(run_path)\n",
    "run_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load masks\n",
    "epochs = config['epochs']\n",
    "k = config['k']\n",
    "n_masks = epochs * BATCHES_PER_EPOCH // k\n",
    "\n",
    "dropout_masks = {}\n",
    "for i in range(n_masks):\n",
    "    dropout_masks[i] = torch.load(f'{run_path}/dropout_mask_{i}.pt')\n",
    "\n",
    "random_masks = {}\n",
    "for i in range(config['n_random_subnets']):\n",
    "    random_masks[i] = torch.load(f'{run_path}/random_mask_{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(epoch):\n",
    "    return torch.load(f'{run_path}/weights_epoch_{epoch}.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment for verifying dropping rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try out dropping out rows and columns\n",
    "w_size = 10\n",
    "dims = [w_size] * 3\n",
    "\n",
    "weights = []\n",
    "for a, b in zip(dims[:-1], dims[1:]):\n",
    "    weights.append(torch.randn(a, b))\n",
    "\n",
    "masks = [(torch.randn(a) > 0).int() for a in dims]\n",
    "input = torch.randn(w_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8566, -0.0000, -0.0000,  0.0000,  0.0000, -0.8037, -0.3888,  0.0000,\n",
       "         0.0000, -1.4470])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = input\n",
    "for w, m in zip(weights, masks[:-1]):\n",
    "    out *= m\n",
    "    out = w @ out\n",
    "out *= masks[-1]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8566,  0.0000,  0.0000,  0.0000,  0.0000, -0.8037, -0.3888,  0.0000,\n",
       "         0.0000, -1.4470])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = []\n",
    "for w, pre_m, post_m in zip(weights, masks[:-1], masks[1:]):\n",
    "    pre_m_expanded = pre_m.view(1, -1).expand_as(w)\n",
    "    post_m_expanded = post_m.view(-1, 1).expand_as(w)\n",
    "\n",
    "    masked_weights.append(w * pre_m_expanded * post_m_expanded)\n",
    "\n",
    "out = input\n",
    "for w, m in zip(masked_weights, masks[:-1]):\n",
    "    out *= m\n",
    "    out = w @ out\n",
    "out *= masks[-1]\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the main experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_weights(w: torch.Tensor, m_pre: torch.Tensor = None, m_post: torch.Tensor = None):\n",
    "    if m_pre is None:\n",
    "        m_pre = torch.ones(w.shape[1])\n",
    "    if m_post is None:\n",
    "        m_post = torch.ones(w.shape[0])\n",
    "\n",
    "    m_pre_expanded = m_pre.view(1, -1).expand_as(w)\n",
    "    m_post_expanded = m_post.view(-1, 1).expand_as(w)\n",
    "    return w * m_pre_expanded * m_post_expanded\n",
    "\n",
    "def get_net_l2(w1: list[torch.Tensor], w2: list[torch.Tensor], m: list[torch.Tensor]):\n",
    "    # we're assuming that each mask immediately precedes its corresponding weight matrix\n",
    "    # TODO: deal with biases\n",
    "\n",
    "    w1_masked = []\n",
    "    for w, m_pre, m_post in zip(w1[:-1], masks[:-1], masks[1:]):\n",
    "        w1_masked.append(get_masked_weights(w, m_pre, m_post))\n",
    "    w1_masked.append(get_masked_weights(w1[-1], m_pre=masks[-1]))\n",
    "    w2_masked = []\n",
    "    for w, m_pre, m_post in zip(w2[:-1], masks[:-1], masks[1:]):\n",
    "        w2_masked.append(get_masked_weights(w, m_pre, m_post))\n",
    "    w2_masked.append(get_masked_weights(w2[-1], m_pre=masks[-1]))\n",
    "\n",
    "    return sum([torch.norm(w1 - w2) for w1, w2 in zip(w1_masked, w2_masked)]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.740933895111084"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: test out get_net_l2\n",
    "\n",
    "w1 = load_weights(1)[::2]\n",
    "w2 = load_weights(3)[::2]\n",
    "masks = dropout_masks[0]\n",
    "\n",
    "get_net_l2(w1, w2, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1680254462.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [11], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    # TODO\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def plot_l2(masks: list[torch.Tensor]):\n",
    "    TYPES = ('dropout', 'random')\n",
    "    if type not in TYPES:\n",
    "        raise ValueError(f'type must be one of {TYPES}')\n",
    "\n",
    "    final_weights = load_weights(epochs)\n",
    "\n",
    "    for i in range(final_weights):\n",
    "\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
